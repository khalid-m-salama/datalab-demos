{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Babyweight Estimation with Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow                         1.10.0     \n",
      "tensorflow-hub                     0.1.0      \n",
      "tensorflow-model-analysis          0.6.0      \n",
      "tensorflow-transform               0.8.0      \n",
      "apache-beam                        2.5.0      \n",
      "google-cloud-dataflow              2.5.0      \n"
     ]
    }
   ],
   "source": [
    "!pip list | grep 'tensorflow'\n",
    "!pip list | grep 'beam'\n",
    "!pip list | grep 'cloud-dataflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'ksalama-gcs-cloudml'\n",
    "PROJECT = 'ksalama-gcp-playground'\n",
    "REGION = 'europe-west1'\n",
    "ROOT_DIR =  'babyweight_tft'\n",
    "RUN_LOCAL = True\n",
    "\n",
    "OUTPUT_DIR = ROOT_DIR if RUN_LOCAL==True else \"gs://{}/{}\".format(BUCKET,ROOT_DIR)\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(OUTPUT_DIR,'transform')\n",
    "TRANSFORMED_DATA_DIR = os.path.join(OUTPUT_DIR,'transformed')\n",
    "TEMP_DIR = os.path.join(OUTPUT_DIR, 'tmp')\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR,'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema({'_column_schemas': {u'mother_race': ColumnSchema({'_domain': IntDomain({'_min_value': -1, '_max_value': 10, '_vocabulary_file': u'', '_is_categorical': True, '_dtype': tf.int64}), '_axes': [], '_representation': FixedColumnRepresentation(None)}), u'weight_pounds': ColumnSchema({'_domain': FloatDomain({'_dtype': tf.float32}), '_axes': [], '_representation': FixedColumnRepresentation(None)}), u'is_male': ColumnSchema({'_domain': IntDomain({'_min_value': -1, '_max_value': 1, '_vocabulary_file': u'', '_is_categorical': True, '_dtype': tf.int64}), '_axes': [], '_representation': FixedColumnRepresentation(None)}), u'gestation_weeks_scaled': ColumnSchema({'_domain': FloatDomain({'_dtype': tf.float32}), '_axes': [], '_representation': FixedColumnRepresentation(None)}), u'mother_age_normalized': ColumnSchema({'_domain': FloatDomain({'_dtype': tf.float32}), '_axes': [], '_representation': FixedColumnRepresentation(None)}), u'mother_age_log': ColumnSchema({'_domain': FloatDomain({'_dtype': tf.float32}), '_axes': [], '_representation': FixedColumnRepresentation(None)}), u'mother_age_bucketized': ColumnSchema({'_domain': IntDomain({'_min_value': 0, '_max_value': 4, '_vocabulary_file': u'', '_is_categorical': True, '_dtype': tf.int64}), '_axes': [], '_representation': FixedColumnRepresentation(None)}), u'is_multiple': ColumnSchema({'_domain': IntDomain({'_min_value': -9223372036854775808, '_max_value': 9223372036854775807, '_vocabulary_file': u'', '_is_categorical': False, '_dtype': tf.int64}), '_axes': [], '_representation': FixedColumnRepresentation(None)})}})\n"
     ]
    }
   ],
   "source": [
    "transformed_metadata = transformed_metadata = metadata_io.read_metadata(\n",
    "        os.path.join(TRANSFORM_ARTEFACTS_DIR,\"transformed_metadata\"))\n",
    "\n",
    "TARGET_FEATURE_NAME = 'weight_pounds'\n",
    "\n",
    "print transformed_metadata.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecords_input_fn(files_name_pattern, transformed_metadata,\n",
    "                       mode=tf.estimator.ModeKeys.EVAL,  \n",
    "                       num_epochs=1, \n",
    "                       batch_size=500):\n",
    "    \n",
    "    dataset = tf.contrib.data.make_batched_features_dataset(\n",
    "        file_pattern=files_name_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_metadata.schema.as_feature_spec(),\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=True if mode == tf.estimator.ModeKeys.TRAIN else False,\n",
    "        shuffle_buffer_size=1+(batch_size*2),\n",
    "        prefetch_buffer_size=1\n",
    "    )\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features = iterator.get_next()\n",
    "    target = features.pop(TARGET_FEATURE_NAME)\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_IdentityCategoricalColumn(key=u'mother_race', num_buckets=11, default_value=None), _IdentityCategoricalColumn(key=u'is_male', num_buckets=2, default_value=None), _IdentityCategoricalColumn(key=u'mother_age_bucketized', num_buckets=5, default_value=None), _CrossedColumn(keys=('mother_age_bucketized', 'mother_race_index'), hash_bucket_size=55, hash_key=None)]\n",
      "\n",
      "[_NumericColumn(key=u'gestation_weeks_scaled', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), _NumericColumn(key=u'mother_age_normalized', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), _NumericColumn(key=u'mother_age_log', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), _EmbeddingColumn(categorical_column=_CrossedColumn(keys=('mother_age_bucketized', 'mother_race_index'), hash_bucket_size=55, hash_key=None), dimension=5, combiner='mean', layer_creator=<function _creator at 0x122985488>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)]\n"
     ]
    }
   ],
   "source": [
    "def create_wide_and_deep_feature_columns(transformed_metadata, extend=False):\n",
    "    \n",
    "    deep_feature_columns = []\n",
    "    wide_feature_columns = []\n",
    "    \n",
    "    column_schemas = transformed_metadata.schema.column_schemas\n",
    "    for feature_name in column_schemas:\n",
    "        if feature_name == TARGET_FEATURE_NAME:\n",
    "            continue\n",
    "        column_schema = column_schemas[feature_name]\n",
    "        if isinstance(column_schema._domain, dataset_schema.FloatDomain):\n",
    "            deep_feature_columns.append(tf.feature_column.numeric_column(feature_name))\n",
    "        elif isinstance(column_schema._domain, dataset_schema.IntDomain):\n",
    "            if column_schema._domain._is_categorical:\n",
    "                wide_feature_columns.append(\n",
    "                    tf.feature_column.categorical_column_with_identity(\n",
    "                        feature_name, \n",
    "                        num_buckets=column_schema._domain._max_value+1)\n",
    "                )\n",
    "                \n",
    "     \n",
    "    if extend==True:\n",
    "        mother_race_X_mother_age_bucketized = tf.feature_column.crossed_column(\n",
    "            ['mother_age_bucketized', 'mother_race_index'],  55)\n",
    "        \n",
    "        wide_feature_columns.append(mother_race_X_mother_age_bucketized)\n",
    "        \n",
    "        mother_race_X_mother_age_bucketized_embedded = tf.feature_column.embedding_column(mother_race_X_mother_age_bucketized, 5)\n",
    "        deep_feature_columns.append(mother_race_X_mother_age_bucketized_embedded)\n",
    "        \n",
    "    return wide_feature_columns, deep_feature_columns\n",
    "\n",
    "wide,deep = create_wide_and_deep_feature_columns(transformed_metadata, True)\n",
    "print wide\n",
    "print \"\"\n",
    "print deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(run_config, hparams):\n",
    "  \n",
    "    wide_feature_columns, deep_feature_columns = create_wide_and_deep_feature_columns(transformed_metadata, \n",
    "                                                                                      hparams.extend_feature_columns)\n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "                linear_feature_columns = wide_feature_columns,\n",
    "                dnn_feature_columns = deep_feature_columns,\n",
    "                dnn_hidden_units=hparams.hidden_units,\n",
    "                config = run_config\n",
    "                )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs=10,\n",
    "    batch_size=500,\n",
    "    hidden_units=[32, 16],\n",
    "    max_steps=100,\n",
    "    extend_feature_columns=False,\n",
    "    evaluate_after_sec=10\n",
    ")\n",
    "\n",
    "model_dir = os.path.join(MODELS_DIR,\"dnn_estimator\")\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    model_dir=model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_files = os.path.join(TRANSFORMED_DATA_DIR, \"train-*.tfrecords\")\n",
    "eval_data_files = os.path.join(TRANSFORMED_DATA_DIR, \"eval-*.tfrecords\")\n",
    "\n",
    "# TrainSpec\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "  input_fn = lambda: tfrecords_input_fn(train_data_files,transformed_metadata,\n",
    "    mode=tf.estimator.ModeKeys.TRAIN,\n",
    "    num_epochs= hparams.num_epochs,\n",
    "    batch_size = hparams.batch_size\n",
    "  ),\n",
    "  max_steps=hparams.max_steps,\n",
    ")\n",
    "\n",
    "# EvalSpec\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "  input_fn =lambda: tfrecords_input_fn(eval_data_files,transformed_metadata),\n",
    "  steps = None,\n",
    "  throttle_secs = hparams.evaluate_after_sec # evalute after each 10 training seconds!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_global_id_in_cluster': 0, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 19830610, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x122d0c910>, '_model_dir': 'babyweight_tft/models/dnn_estimator', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_service': None, '_device_fn': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\n",
      "\n",
      "Experiment started at 14:34:13\n",
      ".......................................\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into babyweight_tft/models/dnn_estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 22315.299, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into babyweight_tft/models/dnn_estimator/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-28-14:34:17\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from babyweight_tft/models/dnn_estimator/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-28-14:34:17\n",
      "INFO:tensorflow:Saving dict for global step 100: average_loss = 32.86104, global_step = 100, label/mean = 7.3133, loss = 16430.52, prediction/mean = 1.7269906\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: babyweight_tft/models/dnn_estimator/model.ckpt-100\n",
      "INFO:tensorflow:Loss for final step: 14955.802.\n",
      ".......................................\n",
      "Experiment finished at 14:34:17\n",
      "\n",
      "Experiment elapsed time: 4.627896 seconds\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if tf.gfile.Exists(model_dir):\n",
    "    tf.gfile.DeleteRecursively(model_dir)\n",
    "\n",
    "estimator = create_estimator(run_config, hparams)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow() \n",
    "print(\"\")\n",
    "print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\") \n",
    "\n",
    "\n",
    "tf.estimator.train_and_evaluate(\n",
    "  estimator,\n",
    "  train_spec,\n",
    "  eval_spec\n",
    ")\n",
    "\n",
    "\n",
    "time_end = datetime.utcnow() \n",
    "print(\".......................................\")\n",
    "print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'gestation_weeks': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'is_male': FixedLenFeature(shape=[], dtype=tf.string, default_value=None),\n",
      "    'key': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'mother_age': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'mother_race': FixedLenFeature(shape=[], dtype=tf.string, default_value=None),\n",
      "    'plurality': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'weight_pounds': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None)}\n"
     ]
    }
   ],
   "source": [
    "CATEGORICAL_FEATURE_NAMES = ['is_male', 'mother_race']\n",
    "NUMERIC_FEATURE_NAMES = ['mother_age', 'plurality', 'gestation_weeks']\n",
    "TARGET_FEATURE_NAME = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "def create_raw_metadata():  \n",
    "    \n",
    "    raw_data_schema = {}\n",
    "    \n",
    "    # key feature scehma\n",
    "    raw_data_schema[KEY_COLUMN]= dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "    \n",
    "    # target feature scehma\n",
    "    raw_data_schema[TARGET_FEATURE_NAME]= dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "    \n",
    "    # categorical features scehma\n",
    "    raw_data_schema.update({ column_name : dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in CATEGORICAL_FEATURE_NAMES})\n",
    "    \n",
    "    # numerical features scehma\n",
    "    raw_data_schema.update({ column_name : dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in NUMERIC_FEATURE_NAMES})\n",
    "    \n",
    "      # create dataset_metadata given raw_schema\n",
    "    raw_metadata = dataset_metadata.DatasetMetadata(\n",
    "        dataset_schema.Schema(raw_data_schema))\n",
    "    \n",
    "    return raw_metadata\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(create_raw_metadata().schema.as_feature_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Estimator to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:partially_apply_saved_transform is deprecated.  Use the transform_raw_features method of the TFTrandformOutput class instead.\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_2:0\\022\\013mother_race\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_6:0\\022\\007is_male\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'gestation_weeks': <tf.Tensor 'gestation_weeks:0' shape=(?,) dtype=float32>, 'is_male': <tf.Tensor 'is_male:0' shape=(?,) dtype=string>, 'mother_race': <tf.Tensor 'mother_race:0' shape=(?,) dtype=string>, 'plurality': <tf.Tensor 'plurality:0' shape=(?,) dtype=float32>, 'mother_age': <tf.Tensor 'mother_age:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'gestation_weeks': <tf.Tensor 'gestation_weeks:0' shape=(?,) dtype=float32>, 'is_male': <tf.Tensor 'is_male:0' shape=(?,) dtype=string>, 'mother_race': <tf.Tensor 'mother_race:0' shape=(?,) dtype=string>, 'plurality': <tf.Tensor 'plurality:0' shape=(?,) dtype=float32>, 'mother_age': <tf.Tensor 'mother_age:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from babyweight_tft/models/dnn_estimator/model.ckpt-100\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: babyweight_tft/models/dnn_estimator/export/temp-1535468534/assets\n",
      "INFO:tensorflow:SavedModel written to: babyweight_tft/models/dnn_estimator/export/temp-1535468534/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'babyweight_tft/models/dnn_estimator/export/1535468534'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \n",
    "    from tensorflow_transform.saved import saved_transform_io\n",
    "    \n",
    "    # get the feature_spec of raw data\n",
    "    raw_metadata = create_raw_metadata()\n",
    "    raw_input_features = raw_metadata.schema.as_batched_placeholders()\n",
    "    raw_input_features.pop(TARGET_FEATURE_NAME)\n",
    "    raw_input_features.pop(KEY_COLUMN)\n",
    "\n",
    "    # apply tranform_fn on raw features\n",
    "    _, transformed_features = (\n",
    "        saved_transform_io.partially_apply_saved_transform(\n",
    "            os.path.join(TRANSFORM_ARTEFACTS_DIR,transform_fn_io.TRANSFORM_FN_DIR),\n",
    "        raw_input_features)\n",
    "    )\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        transformed_features, raw_input_features)\n",
    "\n",
    "\n",
    "\n",
    "export_dir = os.path.join(model_dir, 'export')\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "    tf.gfile.DeleteRecursively(export_dir)\n",
    "        \n",
    "estimator.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=serving_input_receiver_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babyweight_tft/models/dnn_estimator/export/1535468014\n",
      "INFO:tensorflow:Restoring parameters from babyweight_tft/models/dnn_estimator/export/1535468014/variables/variables\n",
      "1.7166116\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir=os.path.join(export_dir, os.listdir(export_dir)[0])\n",
    "\n",
    "print saved_model_dir\n",
    "\n",
    "def estimate_local(instance):\n",
    " \n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "        export_dir=saved_model_dir,\n",
    "        signature_def_key=\"predict\"\n",
    "    )\n",
    "    \n",
    "    instance = dict((k, [v]) for k, v in instance.items())\n",
    "    value = predictor_fn(instance)['predictions'][0][0]\n",
    "    return value\n",
    "\n",
    "instance = {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39\n",
    "}\n",
    "\n",
    "prediction = estimate_local(instance)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babyweight_tft/models/dnn_estimator/export/1535468014\n",
      "assets\n",
      "saved_model.pb\n",
      "variables\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['predict']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['gestation_weeks'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: gestation_weeks:0\n",
      "    inputs['is_male'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: is_male:0\n",
      "    inputs['mother_age'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: mother_age:0\n",
      "    inputs['mother_race'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: mother_race:0\n",
      "    inputs['plurality'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: plurality:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['predictions'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: add:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=babyweight_tft/models/dnn_estimator/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}\n",
    "saved_model_cli show --dir=${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
